{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab Notebook - Section 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DHH-1YKcIuRW",
        "0LKMid597qjL",
        "7URUz-xz9otD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXML2GuPHxvg"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>\n",
        "\n",
        "<h1>Applied Text Mining and Sentiment Analysis with Python</h1>\n",
        "\n",
        "Welcome! \n",
        "\n",
        "In this course we will discover different applications that can be made of text mining. We are going to use this knowledge to focus on a particular NLP application ... that may be a little less known: Sentiment Analysis! \n",
        "\n",
        "So, if you're ready. Let's go!\n",
        "\n",
        "**PS**: don't hesitate to visit **www.AIOutsider.com** for more content!\n",
        "\n",
        "**PS2**: sometimes, the runtime of Colab might expire. If you see an error, always make sure all the code in previous cells has run correctly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H4YR66DPimn"
      },
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
        "\n",
        "# `Section 1` Dataset Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV1N5kiacX1Z"
      },
      "source": [
        "\r\n",
        "In this section, we are going to have a look at the dataset that we will use all along the course.\r\n",
        "\r\n",
        "***Note***: for optimal course follow-up, you will need a Google Account with the Google Drive functionnality. If you don't have a Google Account, you can create a new one (it's totally *free*) or simply download the dataset ressource and manually upload it to the current session. \r\n",
        "\r\n",
        "If you prefer not to use any Google apps for this course, it's totally fine ... You can simply download this notebook together with the dataset and use them with your favourite IDE (Pycharm / Spyder / Jupyter Notebook / ...)! No constraint here 🙃\r\n",
        "\r\n",
        "... In any case, enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0pwA66BACFH"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **1.1** Connect to Google Drive\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOIlGVUUm-n"
      },
      "source": [
        "* Initiate the connection with Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhLkmUkO59g"
      },
      "source": [
        "# Import PyDrive and associated libraries\n",
        "# This only needs to be done once per notebook\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "# This only needs to be done once per notebook\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SESTuTU_T8WO"
      },
      "source": [
        "* Specify the Google Drive file ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxz1za_qBYRt"
      },
      "source": [
        "# Download a file based on its file ID.\r\n",
        "\r\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\r\n",
        "file_id = 'YOUR_FILE_ID' # Check your own ID in GDrive\r\n",
        "downloaded = drive.CreateFile({'id': file_id})\r\n",
        "\r\n",
        "# Save file in Colab memory\r\n",
        "downloaded.GetContentFile('tweet_data.csv')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8IZvvYyA9sr"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **1.2** Load and analyze Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1SvV_1MAXMq"
      },
      "source": [
        "Very useful packages for Data Analysis. Don't hesitate to visit any of those sites in case you don't get what we are doing.\n",
        "*   `Pandas`: https://pandas.pydata.org/docs/\n",
        "*   `Numpy`: https://numpy.org/doc/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K4rZaHvkJhd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmpTLrUWAqFI"
      },
      "source": [
        "* Read Dataframe stored in Google Drive under `.csv` format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMVYdAbhkSEv"
      },
      "source": [
        "df = pd.read_csv(\"tweet_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxeev34FBIJH"
      },
      "source": [
        "* Use the `sample` method to look at some random tweets present in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ePfKnQkiHh"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKPIS_XzCCkO"
      },
      "source": [
        "* Check how many tweets there are in total"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d8KD2RCzBMv"
      },
      "source": [
        "print(\"Number of tweets: {}\".format(len(df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US5Uqnn4V1RT"
      },
      "source": [
        "* Print a tweet and its sentiment based on a tweet ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeDUPoASy8Bh"
      },
      "source": [
        "tweet_id = 4879\n",
        "tweet = df.iloc[tweet_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKuB8wZ4ftXE"
      },
      "source": [
        "print(\"Tweet: {}\".format(tweet[\"tweet_text\"]))\r\n",
        "print(\"Tweet sentiment: {}\".format(tweet[\"sentiment\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2QDfi2dWWFK"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **1.3** Dataset Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDpxKjeZQcKE"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **1.3.1** Matplotlib</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DPCRFyGCuMc"
      },
      "source": [
        "* Import the `pyplot` module from the matplotlib package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb79SIovB06O"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sMh0LQRV_Da"
      },
      "source": [
        "* `pyplot` helps understanding and representing how tweets are distributed over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya7k2TcylAbp"
      },
      "source": [
        "sentiment_count = df[\"sentiment\"].value_counts()\r\n",
        "plt.pie(sentiment_count, labels=sentiment_count.index,\r\n",
        "        autopct='%1.1f%%', shadow=True, startangle=140)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFvGFDbFRZGK"
      },
      "source": [
        "* Print the count of positive and negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlwWqCohC87z"
      },
      "source": [
        "print(\"Number of + tweets: {}\".format(df[df[\"sentiment\"]==\"positive\"].count()[0]))\n",
        "print(\"Number of - tweets: {}\".format(df[df[\"sentiment\"]==\"negative\"].count()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXAMS4bQjGM"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **1.3.2** Wordclouds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTIV2ElsBhZk"
      },
      "source": [
        "* The `Wordclouds` package is very useful to get a quick overview of most recurrent words in the text corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiME9k4uBxix"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stf9-UtnNloU"
      },
      "source": [
        "* What are the words most often present in positive tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW3YNWHwwYdP"
      },
      "source": [
        "pos_tweets = df[df[\"sentiment\"]==\"positive\"]\r\n",
        "txt = \" \".join(tweet.lower() for tweet in pos_tweets[\"tweet_text\"])\r\n",
        "wordcloud = WordCloud().generate(txt)\r\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7FsFmH6NqWy"
      },
      "source": [
        "* ... and in negative tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbHSPX8jE2Jk"
      },
      "source": [
        "neg_tweets = df[df[\"sentiment\"]==\"negative\"]\r\n",
        "txt = \" \".join(tweet.lower() for tweet in neg_tweets[\"tweet_text\"])\r\n",
        "wordcloud = WordCloud().generate(txt)\r\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlYJCg2vSuA_"
      },
      "source": [
        "That's it for this section! \r\n",
        "\r\n",
        "See you in `Section 2` where we will talk about Text Normalization and get our hands dirty with some real tweets!\r\n",
        "\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIFAbVfhMvKS"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
        "\n",
        "# `SECTION 2` Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56HdmPCjMjls"
      },
      "source": [
        "As this section relies quite a lot on `REGEX`, you might want to have a look at the following website: https://regexr.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D9TW-jHaDMY"
      },
      "source": [
        "* Import `regex` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qefx0Ad1RFHy"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKiiuRQpdxjm"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **2.1** Twitter features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1N3d1u49GUt"
      },
      "source": [
        "* Example of a *random* tweet that can be found on Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8qaWEnedWB1"
      },
      "source": [
        "tweet = \"RT @AIOutsider I love this! 👍 https://AIOutsider.com #NLP #Fun\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaKqk-_gFQQN"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.1** RT Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHH-1YKcIuRW"
      },
      "source": [
        "#### Need a hint?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZVeoDQK6kvr"
      },
      "source": [
        "* `R` : match \"R\" character\r\n",
        "* `T` : match \"T\" character\r\n",
        "* `\\s` : match any whitespace character\r\n",
        "* `+` : match one or more of the preceding tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMrR3STEfdrA"
      },
      "source": [
        "#### Handle the RT Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5R1W_qw3AZs"
      },
      "source": [
        "* Replace occurences of `RT` with a default value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMe1sJQZWBon"
      },
      "source": [
        "def replace_retweet(tweet, default_replace=\"\"):\r\n",
        "  tweet = re.sub('RT\\s+', default_replace, tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIFwuvPQI5L4"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_retweet(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY48gGm2fgtV"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.2** @User Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LKMid597qjL"
      },
      "source": [
        "#### Need a hint?\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc0j6Mfw8Tcw"
      },
      "source": [
        "* `\\B` : match any position that is not a word boundary\r\n",
        "* `@` : match \"@\" character\r\n",
        "* `\\w` : match any word character \r\n",
        "* `+` : match one or more of the preceding tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjn3WP56Dc-O"
      },
      "source": [
        "#### Handle the User Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DMdteLL3Sq-"
      },
      "source": [
        "* Replace `@_Someone_` with a default user tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODnCCX1ueIwO"
      },
      "source": [
        "def replace_user(tweet, default_replace=\"twitteruser\"):\r\n",
        "  tweet = re.sub('\\B@\\w+', default_replace, tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgSircywI8fG"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_user(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsbZqz2Cn1dB"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.3** Emojis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE5IZIx5Gkx4"
      },
      "source": [
        "* Install the `emoji` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDtbHjDiiaw7"
      },
      "source": [
        "pip install emoji --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTSClFfP3iXm"
      },
      "source": [
        "* Import the installed package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ctx6FUiVyc"
      },
      "source": [
        "import emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZp0rcEGtH8"
      },
      "source": [
        "* Replace emojis with a meaningful text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqHXnwRZn2Ey"
      },
      "source": [
        "def demojize(tweet):\r\n",
        "  tweet = emoji.demojize(tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuCWELwx2bb"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(demojize(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSYc3ax5fk5m"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.4** URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7URUz-xz9otD"
      },
      "source": [
        "#### Need a hint?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKaWmYHL4bmw"
      },
      "source": [
        "* `(http|https)` : capturing group matching either http or https\r\n",
        "* `:` : match the \":\" character\r\n",
        "* `\\/` : match the \"/\" charachter\r\n",
        "* `\\S` : match any character that is not whitespace\r\n",
        "* `+` : match one or more of the preceding tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1_YIxt5z3d"
      },
      "source": [
        "#### Handle the URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp8E1W-e4QDT"
      },
      "source": [
        "* Replace occurences of `http://` or `https://` with a default value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vq3d5VcfdCc"
      },
      "source": [
        "def replace_url(tweet, default_replace=\"\"):\r\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN2c4pJIy8pX"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_url(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKIdmGCimZ1"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.5** Hashtags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yePSPCRy4WWk"
      },
      "source": [
        "* Replace occurences of `#_something_` with a default value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI76Ve3lg3WZ"
      },
      "source": [
        "def replace_hashtag(tweet, default_replace=\"\"):\r\n",
        "  tweet = re.sub('#+', default_replace, tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xlMKyD4y-Xg"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_hashtag(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77D-JqISVFvi"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
        "\n",
        "## **2.2** Word Features\n",
        "\n",
        "Let's now have a look at some other features that are not really Twitter-dependant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw18TAaFhOMH"
      },
      "source": [
        "tweet = \"LOOOOOOOOK at this ... I'd like it so much!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmN1OTYBi8r3"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.1** Remove upper capitalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noY_ATKH4hcz"
      },
      "source": [
        "* Lower case each letter in a specific tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXCITDNeQtnE"
      },
      "source": [
        "def to_lowercase(tweet):\r\n",
        "  tweet = tweet.lower()\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Roo8SHo23vw"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(to_lowercase(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNlZxI4ZS-dX"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.2** Word repetition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKkeEGaX4m40"
      },
      "source": [
        "* Replace word repetition with a single occurence (\"oooooo\" becomes \"oo\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USLuHaaiS9xm"
      },
      "source": [
        "def word_repetition(tweet):\r\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkWh38hH5qyY"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(word_repetition(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qli_4tnBpcr4"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.3** Punctuation repetition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQN36n3w4wNA"
      },
      "source": [
        "* Replace punctuation repetition with a single occurence (\"!!!!!\" becomes \"!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_awIiJ_ngo7"
      },
      "source": [
        "def punct_repetition(tweet, default_replace=\"\"):\r\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfRfogEe5str"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(punct_repetition(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpwgMqIxi6G9"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.4** Word contraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adsu8A-TGFT5"
      },
      "source": [
        "* Install the `contractions` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO2Gle7Ey360"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOPWYcd146Rl"
      },
      "source": [
        "* Import the installed package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdkPTEZryzJC"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDGBT323GE1v"
      },
      "source": [
        "* Use `contractions_dict` to list most common contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DwhVxKs1QL_"
      },
      "source": [
        "print(contractions.contractions_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lho1c8I5D2B"
      },
      "source": [
        "* Create a `_fix_contractions` function used to replace contractions with their extended forms by using the contractions dictionnary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xj5UXbVspq"
      },
      "source": [
        "def _fix_contractions(tweet):\r\n",
        "  for k, v in contractions.contractions_dict.items():\r\n",
        "    tweet = tweet.replace(k, v)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MDFOXKTccRV"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(_fix_contractions(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQijW6z65IzJ"
      },
      "source": [
        "* Create a `_fix_contractions` function used to replace contractions with their extended forms by using the contractions package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6atIK-VsNw"
      },
      "source": [
        "def fix_contractions(tweet):\r\n",
        "  tweet = contractions.fix(tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqQ60KaMc0CS"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(fix_contractions(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Qnxa-_2q6x"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **2.3** Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eECnMHD_89wz"
      },
      "source": [
        "More information about NLTK? Head over to: https://www.nltk.org\r\n",
        "\r\n",
        "* Install the `NLTK` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnKD-MkZ2kNd"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8KTZgynDSTg"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.3.1** Easy Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO_sTCV19KA5"
      },
      "source": [
        "* Import `NLTK`\r\n",
        "* Import the `word_tokenize` module from NLTK \r\n",
        "* Download the `Punkt` tokenizer model from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6SVKvVP4NDL"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCtkPkWIEXdl"
      },
      "source": [
        "* Simple tweet to be tokenized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJLYCxJztrXo"
      },
      "source": [
        "tweet = \"These are 5 different words!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0nDxqUDy6s"
      },
      "source": [
        "* Create a `tokenize()` function that takes a tweet as input and returns a list of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MthNx7ou4OnW"
      },
      "source": [
        "def tokenize(tweet):\r\n",
        "  tokens = word_tokenize(tweet)\r\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoiTPGFNDqAf"
      },
      "source": [
        "* Use the `tokenize()` function to print the tokenized version of a tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPdJeqcRCYT2"
      },
      "source": [
        "print(type(tokenize(tweet)))\r\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Ha_2Dtjl6X"
      },
      "source": [
        "That's it for the easy method! You will now see there is more you can do about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8KrFT2jDaVj"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.3.2** Custom Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLPhIt14FfVc"
      },
      "source": [
        "* Import the `string` package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baAGSohY5PcV"
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTwnZoFYTrpN"
      },
      "source": [
        "* Retrieve english punctuation signs by using the `string` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc6naeSUkNs8"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KM4_kdUFsP"
      },
      "source": [
        "* Import the `stopwords` module from NLTK\r\n",
        "* Download `stopwords` data from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MBBQJDeQSi4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXQ8YUNj5-B1"
      },
      "source": [
        "* Create a set of english stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXqXrG42qMk"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\r\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9G7T6Wg6A5F"
      },
      "source": [
        "* Remove some stopwords from the set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcgpw29wmec4"
      },
      "source": [
        "stop_words.discard('not')\r\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4LLuBGH6Dy5"
      },
      "source": [
        "* Create a `custom_tokenize` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6cZbaTB5Yr7"
      },
      "source": [
        "def custom_tokenize(tweet,\r\n",
        "                    keep_punct = False,\r\n",
        "                    keep_alnum = False,\r\n",
        "                    keep_stop = False):\r\n",
        "  \r\n",
        "  token_list = word_tokenize(tweet)\r\n",
        "\r\n",
        "  if not keep_punct:\r\n",
        "    token_list = [token for token in token_list\r\n",
        "                  if token not in string.punctuation]\r\n",
        "\r\n",
        "  if not keep_alnum:\r\n",
        "    token_list = [token for token in token_list if token.isalpha()]\r\n",
        "  \r\n",
        "  if not keep_stop:\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "    stop_words.discard('not')\r\n",
        "    token_list = [token for token in token_list if not token in stop_words]\r\n",
        "\r\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z5OuuxY6L9b"
      },
      "source": [
        "* Test the function with a particular tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LDWcJ99E6Z2"
      },
      "source": [
        "tweet = \"these are 5 different words!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXyaftGrEvkC"
      },
      "source": [
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, \r\n",
        "                                                keep_punct=True, \r\n",
        "                                                keep_alnum=True, \r\n",
        "                                                keep_stop=True)))\r\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_stop=True)))\r\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_alnum=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIY4CKCcTyEo"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **2.4** Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoizpNg82nK_"
      },
      "source": [
        "More information about `NLTK.stem`? Head over to: https://www.nltk.org/api/nltk.stem.html\r\n",
        "\r\n",
        "* Import different libraries and modules used for stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oaU_cd_Wz0u"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb9Joc083a2i"
      },
      "source": [
        "* List of tokens to stem (remember that we stem tokens and not entire sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTraXL0yy1Up"
      },
      "source": [
        "tokens = [\"manager\", \"management\", \"managing\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v35wUbss5fnI"
      },
      "source": [
        "* Stemmers can be defined by directly using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8QDL-05tMj5"
      },
      "source": [
        "porter_stemmer = PorterStemmer()\r\n",
        "lancaster_stemmer = LancasterStemmer()\r\n",
        "snoball_stemmer = SnowballStemmer('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5j_BLPU5ptp"
      },
      "source": [
        "* Create a `stem_tokens` function that takes the list of tokens as input and returns a list of stemmed tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CetNdOeRTyjj"
      },
      "source": [
        "def stem_tokens(tokens, stemmer):\r\n",
        "  token_list = []\r\n",
        "  for token in tokens:\r\n",
        "    token_list.append(stemmer.stem(token))\r\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swSFzfQI6PQ3"
      },
      "source": [
        "* Print the different results and compare the stemmed tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDZqSc43tKjI"
      },
      "source": [
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\r\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\r\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H102rFNu6VzW"
      },
      "source": [
        "* Check over-stemming and under-stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9beBm8O0Gpp"
      },
      "source": [
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGhCVph1VDp"
      },
      "source": [
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\r\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\r\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62AsuQu6UIEn"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
        "\n",
        "## **2.5** Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8rZjBEzcaI"
      },
      "source": [
        "More information about WordNet? Head over to https://wordnet.princeton.edu/\r\n",
        "\r\n",
        "* Import different libraries and modules used for lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUdQlhSQUL7T"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgJuRIZf7yWQ"
      },
      "source": [
        "* List of tokens to lemmatize (remember that we lemmatize tokens and not entire sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljxJlz0yJ8H"
      },
      "source": [
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQjP0xoH2NcZ"
      },
      "source": [
        "* Part of Speech (POS) tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWHJMdHH18le"
      },
      "source": [
        "word_type = {\"international\": wordnet.ADJ, \n",
        "             \"companies\": wordnet.NOUN, \n",
        "             \"had\": wordnet.VERB, \n",
        "             \"interns\": wordnet.NOUN\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e30o3j5N72sw"
      },
      "source": [
        "* Create the lemmatizer by using the `WordNet` module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-AAGLN4UWTO"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9pDpdOo78jH"
      },
      "source": [
        "* Create a `lemmatize_tokens` function that takes the list of tokens as input and returns a list of lemmatized tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Kiw9xOZE-F"
      },
      "source": [
        "def lemmatize_tokens(tokens, word_type, lemmatizer):\r\n",
        "  token_list = []\r\n",
        "  for token in tokens:\r\n",
        "    token_list.append(lemmatizer.lemmatize(token, word_type[token]))\r\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nnvMrk_UYZ1"
      },
      "source": [
        "print(\"Tweet lemma: {}\".format(\r\n",
        "    lemmatize_tokens(tokens, word_type, lemmatizer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTervSXj3zkF"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **2.6** Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9YSiUE8I03"
      },
      "source": [
        "* Long and complex tweet to be processed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COBX24hLZiZn"
      },
      "source": [
        "complex_tweet = r\"\"\"RT @AIOutsider : he looooook, \r\n",
        "THis is a big and complex TWeet!!! 👍 ... \r\n",
        "We'd be glad if you couldn't normalize it! \r\n",
        "Check https://t.co/7777 and LET ME KNOW!!! #NLP #Fun\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pRABF3t8MO0"
      },
      "source": [
        "* Create a custom `process_tweet` function that can be used to process tweets end-to-end\r\n",
        "* **Note**: this function will be used as a base for the following sections, so be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQofxMv37Ik"
      },
      "source": [
        "def process_tweet(tweet, verbose=False):\n",
        "  if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Twitter Features\n",
        "  tweet = replace_retweet(tweet) # replace retweet\n",
        "  tweet = replace_user(tweet, \"\") # replace user tag\n",
        "  tweet = replace_url(tweet) # replace url\n",
        "  tweet = replace_hashtag(tweet) # replace hashtag\n",
        "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Word Features\n",
        "  tweet = to_lowercase(tweet) # lower case\n",
        "  tweet = fix_contractions(tweet) # replace contractions\n",
        "  tweet = punct_repetition(tweet) # replace punctuation repetition\n",
        "  tweet = word_repetition(tweet) # replace word repetition\n",
        "  tweet = demojize(tweet) # replace emojis\n",
        "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Tokenization & Stemming\n",
        "  tokens = custom_tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize\n",
        "  stemmer = SnowballStemmer(\"english\") # define stemmer\n",
        "  stem = stem_tokens(tokens, stemmer) # stem tokens\n",
        "\n",
        "  return stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKBv4elc81PC"
      },
      "source": [
        "* Test your `process_tweet` function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xb146QF4uhj"
      },
      "source": [
        "print(process_tweet(complex_tweet, verbose=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy48UzeYHFoP"
      },
      "source": [
        "* Look at some more examples! \n",
        "* **Note:** it's totally possible you encounter some strange tweet processing (happens if the original tweet is initially strangely written) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26lxJnf2K7zd"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBa0HUtM4x0n"
      },
      "source": [
        "for i in range(5):\n",
        "  tweet_id = random.randint(0,len(df))\n",
        "  tweet = df.iloc[tweet_id][\"tweet_text\"]\n",
        "  print(process_tweet(tweet, verbose=True))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haxwoF1IM1f1"
      },
      "source": [
        "That's it for this section! \r\n",
        "\r\n",
        "See you in `Section 3` where we will talk about Text Representation and get close to predicting real tweet sentiment!\r\n",
        "\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ScK4-f_cJB"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
        "\n",
        "# `Section 3` Text Representation </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgEa9cqUzoEK"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **3.1** Processing Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIz-oZSEqJB7"
      },
      "source": [
        "* Install the `Scikit-Learn` package which is very useful for a lot of different ML tasks. \r\n",
        "* **Note:** make sure it is installed and up-to-date (once installed/updated, you might be asked to reload Colab). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oZA6AjqC3X"
      },
      "source": [
        "pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjvFLUXi_Ukv"
      },
      "source": [
        "* Apply `process_tweet` function created in section 2 to the entire DataFrame\r\n",
        "* Convert sentiment to 1 for \"positive\" and 0 for \"negative\" sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0_4R6tNqcAg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YHtN5xwqiGM"
      },
      "source": [
        "* Convert DataFrame to two lists: one for the tweet tokens (X) and one for the tweet sentiment (y)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_1XwdIqgXH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm6k_K81i1cG"
      },
      "source": [
        "print(X)\r\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgcJewZTobFk"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **3.2** Positive/Negative Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBpw9KYPHcz5"
      },
      "source": [
        "* Corpus of tweet tokens used for the first method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VygUsA2VbNQv"
      },
      "source": [
        "corpus = [[\"i\", \"love\", \"nlp\"],\n",
        "          [\"i\", \"miss\", \"you\"],\n",
        "          [\"i\", \"love\", \"you\"],\n",
        "          [\"you\", \"are\", \"happy\", \"to\", \"learn\"],\n",
        "          [\"i\", \"lost\", \"my\", \"computer\"],\n",
        "          [\"i\", \"am\", \"so\", \"sad\"]]\n",
        "\n",
        "sentiment = [1, 0, 1, 1, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdGueKbv9o9"
      },
      "source": [
        "\r\n",
        "* Create a `build_freqs` function used to build a dictionnary with the word and sentiment as index and the count of occurence as value\r\n",
        "\r\n",
        "\r\n",
        "<table style=\"width:100%\">\r\n",
        "  <tr>\r\n",
        "    <th>Word</th>\r\n",
        "    <th>Positive</th>\r\n",
        "    <th>Negative</th>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>love</td>\r\n",
        "    <td>dict[(love, 1)]</td>\r\n",
        "    <td>dict[(love, 0)]</td>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>lost</td>\r\n",
        "    <td>dict[(lost, 1)]</td>\r\n",
        "    <td>dict[(lost, 0)]</td>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>happy</td>\r\n",
        "    <td>dict[(happy, 1)]</td>\r\n",
        "    <td>dict[(happy, 0)]</td>\r\n",
        "  </tr>\r\n",
        "</table>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSSP3EFyp9Nr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpMlVUPPx136"
      },
      "source": [
        "* Build the frequency dictionnary on the corpus by using the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S58DffMwi5vN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb0BvLORtJql"
      },
      "source": [
        "print(freqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64pK3pBlx_Hn"
      },
      "source": [
        "* Build the frequency dictionnary on the entire dataset by using the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vnVv4TOi63Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvafc_1fsktr"
      },
      "source": [
        "print(\"Frequency of word 'love' in + tweets: {}\".format(freqs_all[(\"love\", 1)]))\n",
        "print(\"Frequency of word 'love' in - tweets: {}\".format(freqs_all[(\"love\", 0)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KosMwShRyUTW"
      },
      "source": [
        "* Create a `tweet_to_freqs` function used to convert tweets to a 2-d array by using the frequency dictionnary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QoyYL6wtTBP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjGynFT_JOKY"
      },
      "source": [
        "* Print the 2-d vector by using the `tweet_to_freqs` function and the *corpus* dictionnary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nrowt5T0ZQl"
      },
      "source": [
        "print(tweet_to_freq([\"i\", \"love\", \"nlp\"], freqs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0csoh9UJWgP"
      },
      "source": [
        "* Print the 2-d vector by using the `tweet_to_freqs` function and the *dataset* dictionnary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTmNWLcD2QGn"
      },
      "source": [
        "print(tweet_to_freq([\"i\", \"love\", \"nlp\"], freqs_all))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsLNWRrh2XQ9"
      },
      "source": [
        "* Plot word vectors in a chart and see where they locate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL3E5vRrxJrZ"
      },
      "source": [
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "word1 = \"happi\"\n",
        "word2 = \"sad\"\n",
        "\n",
        "def word_features(word, freqs):\n",
        "  x = np.zeros((2,))\n",
        "  if (word, 1) in freqs:\n",
        "    x[0] = np.log(freqs[(word, 1)] + 1)\n",
        "  if (word, 0) in freqs:\n",
        "    x[1] = np.log(freqs[(word, 0)] + 1)\n",
        "  return x\n",
        "\n",
        "x_axis = [word_features(word, freqs_all)[0] for word in [word1, word2]]\n",
        "y_axis = [word_features(word, freqs_all)[1] for word in [word1, word2]]\n",
        "\n",
        "ax.scatter(x_axis, y_axis)  \n",
        "\n",
        "plt.xlabel(\"Log Positive count\")\n",
        "plt.ylabel(\"Log Negative count\")\n",
        "\n",
        "ax.plot([0, 9], [0, 9], color = 'red')\n",
        "plt.text(x_axis[0], y_axis[0], word1)\n",
        "plt.text(x_axis[1], y_axis[1], word2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a_uGCpQ_nu0"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **3.3** Bag of Word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh-4DbOFJlAn"
      },
      "source": [
        "* Corpus of tweet tokens used for the second method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SomLbPUibDa9"
      },
      "source": [
        "corpus = [[\"love\", \"nlp\"],\n",
        "          [\"miss\", \"you\"],\n",
        "          [\"hate\", \"hate\", \"hate\", \"love\"],\n",
        "          [\"happy\", \"love\", \"hate\"],\n",
        "          [\"i\", \"lost\", \"my\", \"computer\"],\n",
        "          [\"i\", \"am\", \"so\", \"sad\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEe5w7SPso2e"
      },
      "source": [
        "* Import `CountVectorizer` from the Scikit-learn Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjjZkgHCpw_m"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a5WqAGSJ0KN"
      },
      "source": [
        "* Create a `fit_cv` function used to build the Bag-of-Words vectorizer with the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEzNadu6pwYh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z07FwlXUJ7ld"
      },
      "source": [
        "* Use the `fit_cv` function to fit the vectorizer on the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xed7_qlKwoX9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0PJfMk2sthJ"
      },
      "source": [
        "* Get the vectorizer features (matrix columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcalFLxjIEx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-nxkR7VfWii"
      },
      "source": [
        "print(\"There are {} features in this corpus\".format(len(ft)))\n",
        "print(ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IueKWpsswRn"
      },
      "source": [
        "* Convert the corpus to a matrix by using the vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-mcZScBdtb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSf9F_jgKRnr"
      },
      "source": [
        "* Print the matrix shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXm86BaJp0yB"
      },
      "source": [
        "print(\"Matrix shape is: {}\".format()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHWE38UMKVOG"
      },
      "source": [
        "* Convert the matrix to an array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBH3Hy60gQB4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvnrCpxvKaMR"
      },
      "source": [
        "* Transform a new tweet by using the vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLVG9xwdwzuN"
      },
      "source": [
        "new_tweet = [[\"lost\", \"lost\", \"miss\", \"miss\"]]\n",
        "cv_vect.transform(new_tweet).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMuCe5KsxyR_"
      },
      "source": [
        "unknown_tweet = [[\"John\", \"drives\", \"cars\"]]\n",
        "cv_vect.transform(unknown_tweet).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sa_FL6vn79E"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **3.4** Term Frequency – Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOZSwJ1hKyAT"
      },
      "source": [
        "* Corpus of tweet tokens used for the third method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ybyRRPZC-L"
      },
      "source": [
        "corpus = [[\"love\", \"nlp\"],\r\n",
        "          [\"miss\", \"you\"],\r\n",
        "          [\"hate\", \"hate\", \"hate\", \"love\"],\r\n",
        "          [\"happy\", \"love\", \"hate\"],\r\n",
        "          [\"i\", \"lost\", \"my\", \"computer\"],\r\n",
        "          [\"i\", \"am\", \"so\", \"sad\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE5zVtjsK6Ta"
      },
      "source": [
        "* Import `TfidfVectorizer` from the Scikit-learn Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlBvj6pARLH1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQgGfCwEK-mI"
      },
      "source": [
        "* Create a `fit_tfidf` function used to build the TF-IDF vectorizer with the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NUd39ZsRQPI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tac2oPSRLErQ"
      },
      "source": [
        "* Use the `fit_cv` function to fit the vectorizer on the corpus, and transform the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjmZhliftA_Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt1eOml3LMas"
      },
      "source": [
        "* Get the vectorizer features (matrix columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRqHhf3njSEe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tywqkwWPlYpc"
      },
      "source": [
        "print(\"There are {} features in this corpus\".format(len(ft)))\n",
        "print(ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BGtKec9LPNg"
      },
      "source": [
        "* Print the matrix shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtamuqDjlxmE"
      },
      "source": [
        "print(tf_mtx.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usbtYKPCLTBN"
      },
      "source": [
        "* Convert the matrix to an array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVVbtSxqlxVi"
      },
      "source": [
        "tf_mtx.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxWpupYkLVxl"
      },
      "source": [
        "* Transform a new tweet by using the vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87JJ_go4uFO5"
      },
      "source": [
        "new_tweet = [[\"I\", \"hate\", \"nlp\"]]\n",
        "tf_vect.transform(new_tweet).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAgMDeTwSkog"
      },
      "source": [
        "That's it for this section! \r\n",
        "\r\n",
        "See you in `Section 4` where we finally build our sentiment model!\r\n",
        "\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4tg1lhoBxR"
      },
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
        "\n",
        "# `Section 4` Sentiment Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjAoXb4XsxgN"
      },
      "source": [
        "## Helper function\r\n",
        "\r\n",
        "This function will be used to plot the confusion matrix for the different models we will create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBiVy9LSnlVN"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "def plot_confusion(cm):\n",
        "  plt.figure(figsize = (5,5))\n",
        "  sn.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f')\n",
        "  plt.xlabel(\"Prediction\")\n",
        "  plt.ylabel(\"True value\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  return sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePstP2McY2Ql"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **4.1** Train/Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfXwu-g4onVk"
      },
      "source": [
        "* Check what X and y looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMqL7yh1Dt8t"
      },
      "source": [
        "print(X)\r\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnJENLcJooVj"
      },
      "source": [
        "* Import the `train_test_split` function from the Scikit-Learn package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkeEyh9TeTT3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgDUiv-po5LZ"
      },
      "source": [
        "* Use the `train_test_split` function to split arrays of X and y into training and testing variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdKNxWTXDvLM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4uKnKwkpG_J"
      },
      "source": [
        "* Print the size of these news variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fUaBFavoBTG"
      },
      "source": [
        "print(\"Size of X_train: {}\".format(len(X_train)))\n",
        "print(\"Size of y_train: {}\".format(len(y_train)))\n",
        "print(\"\\n\")\n",
        "print(\"Size of X_test: {}\".format(len(X_test)))\n",
        "print(\"Size of y_test: {}\".format(len(y_test)))\n",
        "print(\"\\n\")\n",
        "print(\"Train proportion: {:.0%}\".format(len(X_train)/\n",
        "                                        (len(X_train)+len(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVouoqmepP0N"
      },
      "source": [
        "* Print random tweets, just to verify everything goes as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5nWVNZzobfd"
      },
      "source": [
        "id = random.randint(0,len(X_train))\n",
        "print(\"Train tweet: {}\".format(X_train[id]))\n",
        "print(\"Sentiment: {}\".format(y_train[id]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pV5EWbQpIOp"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **4.2** Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvELqpBA62u"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.2.1** Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8DrY2m6L0zq"
      },
      "source": [
        "* Import the `LogisticRegression` model from Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVCZ2jEcb_Kx"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8oqTzbsL5N7"
      },
      "source": [
        "* Create a `fit_lr` function used to fit a Logistic Regression model on X and y *training* data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8x-HdwcC1K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWD8LNbJDHG9"
      },
      "source": [
        "\r\n",
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px>  **4.2.2** Pos/Neg Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bqSbvMxMDk8"
      },
      "source": [
        "* Use the `build_freqs` function on training data to create a frequency dictionnary\r\n",
        "* Use the frequency dictionnary together with the `tweet_to_freq` function to convert X_train and X_test data to 2-d vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDhO2RKBafD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FgQYsodMl1c"
      },
      "source": [
        "* Fit the Logistic Regression model on training data by using the `fit_lr` function\r\n",
        "* Print the model coefficients (betas and intercept)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXgI2QcjYyv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZtyVpIDZBRr"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.2.3** Count Vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-itafPb7M2O-"
      },
      "source": [
        "* Use the `fit_cv` function on training data to build the Bag-of-Words vectorizer\r\n",
        "* Transform X_train and X_test data by using the vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXba4ZPaYsb5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGDgMoYMNGA4"
      },
      "source": [
        "* Fit the Logistic Regression model on training data by using the `fit_lr` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xarmCHAIZMty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7WerftZV4t"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.2.4** TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm27oDZHNMxd"
      },
      "source": [
        "* Use the `fit_cv` function on training data to build the Bag-of-Words vectorizer\r\n",
        "* Transform X_train and X_test data by using the vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Ex6PkLZXwC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdkMm9TWNVTM"
      },
      "source": [
        "* Fit the Logistic Regression model on training data by using the `fit_lr` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfXA_WE7ZgCO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTZWCw7CuAqy"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\r\n",
        "\r\n",
        "## **4.3** Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlWsYS6GNYi8"
      },
      "source": [
        "* Import the `accuracy score` and `confusion matrix` from Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv_HXA9ttu4I"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-OBeBb4vf-Y"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.3.1** Positive/Negative Frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNAY6ENLNgla"
      },
      "source": [
        "* Use the fitted `model_lr_pn` (positive/negative frequencies) to predict X_test\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGSRknqlj6fu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D647C9ORj7Ty"
      },
      "source": [
        "* Print the model accuracy by comparing predictions and real sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Mw_VHwj6Qp"
      },
      "source": [
        "print(\"LR Model Accuracy: {:.2%}\".format())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEtKfgdij-cU"
      },
      "source": [
        "* Plot the confusion matrix by using the `plot_confusion` helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMFVLs_vAO03"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzTQksefvlxJ"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.3.2** Count Vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP_bueksN2cS"
      },
      "source": [
        "* Use the fitted `model_lr_cv` (Bag-of-words) to predict X_test\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suYa_OYgZPq1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpn1YUFokKXm"
      },
      "source": [
        "* Print the model accuracy by comparing predictions and real sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAOncwPekK1_"
      },
      "source": [
        "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_cv)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3_6Fg_zkLVm"
      },
      "source": [
        "* Plot the confusion matrix by using the `plot_confusion` helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlaWIWnNkL1O"
      },
      "source": [
        "plot_confusion(confusion_matrix(y_test, y_pred_lr_cv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TxYFxnduFCH"
      },
      "source": [
        "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.3.3** TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iZdLPwcOAk8"
      },
      "source": [
        "* Use the fitted `model_lr_tf` (TF-IDF) to predict X_test\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4MmLzcCZvto"
      },
      "source": [
        "y_pred_lr_tf = model_lr_tf.predict(X_test_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hn_RKShkVcm"
      },
      "source": [
        "* Print the model accuracy by comparing predictions and real sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6mO1eJkV3m"
      },
      "source": [
        "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_tf)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPojrJ56kWKL"
      },
      "source": [
        "* Plot the confusion matrix by using the `plot_confusion` helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt_FBSmbkW0b"
      },
      "source": [
        "plot_confusion(confusion_matrix(y_test, y_pred_lr_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUjQh8vnPld9"
      },
      "source": [
        "## **4.4** Mini-Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FRX-suJOIqH"
      },
      "source": [
        "* Final tweet used to check if the model works as well as expected\r\n",
        "* **Note:** don't hesitate to input your own tweet!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpybhropPsSe"
      },
      "source": [
        "your_tweet = \"\"\"RT @AIOutsider: tune in for more amazing NLP content! \r\n",
        "And don't forget to visit https://AIOutsider.com ...\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nmPPbkPOSda"
      },
      "source": [
        "* Create a `predict_tweet` function used to pre-process, transform and predict tweet sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3LKbEteQOXy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9M-pJ6GOaHw"
      },
      "source": [
        "* ... Predict your tweet sentiment by using the `predict_tweet` function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YThD6uAmMFEZ"
      },
      "source": [
        "predict_tweet(your_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5TjajsKv7Ha"
      },
      "source": [
        "# Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X5JPSycSwZR"
      },
      "source": [
        "That's it for this course! I hope you enjoyed it as much as I did! Most importantly, I hope you learned some new things about Text Mining, NLP and Sentiment Analysis.\r\n",
        "\r\n",
        "See you next time!\r\n",
        "\r\n",
        "**AI_Outsider**\r\n",
        "\r\n",
        "Don't forget to visit https://AiOutsider.com for more!\r\n",
        "\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
      ]
    }
  ]
}